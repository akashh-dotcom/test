<?xml version="1.0" encoding="UTF-8"?>
<sect1 id="ch0032s0003">
   <sect1info>
      <risinfo>
         <risprev>sect1.9781394211319.ch0032s0002</risprev>
         <riscurrent>sect1.9781394211319.ch0032s0003</riscurrent>
         <risnext>sect1.9781394211319.ch0032s0004</risnext>
         <booktitle>HEALTH BEHAVIOR</booktitle>
         <isbn>9781394211319</isbn>
         <chapternumber/>
         <chapterid>ch0032</chapterid>
         <chaptertitle>KEY POINTS</chaptertitle>
         <authorgroup>
            <author>
               <personname>
                  <firstname>Karen</firstname>
                  <surname>Glanz</surname>
               </personname>
            </author>
            <author>
               <personname>
                  <firstname>Barbara K.</firstname>
                  <surname>Rimer</surname>
               </personname>
            </author>
            <author>
               <personname>
                  <firstname>K.</firstname>
                  <surname>Viswanath</surname>
               </personname>
            </author>
         </authorgroup>
         <publisher>
            <publishername>John Wiley &amp; Sons, Inc</publishername>
         </publisher>
         <pubdate>2024</pubdate>
      </risinfo>
      <primaryauthor>
         <personname>
            <firstname>Karen</firstname>
            <surname>Glanz</surname>
         </personname>
      </primaryauthor>
   </sect1info>
   <title>Discussion and Conclusions</title>
   <anchor id="ch0032s0003a0001"/>
   <anchor id="ch0032s0000a0034"/>
   <para id="ch0032s0000p0044">Like every discipline, communication and, specifically, media effects research, has struggled with unresolved challenges. First, the term “effects” in media effects signals fundamental causality, and yet demonstrating causality is no easy task. Many researchers use randomized controlled experiments to test for message effects (e.g., Bigman,<link linkend="ch0032s0000li0010">2014</link>; Hall et al., <link linkend="ch0032s0000li0037">2018</link>), and these studies provide important insights into how messages shape short‐term attitudes and intentions. However, these methods are harder to use to assess effects on longer‐term outcomes like behavior change. This methodological challenge is particularly true in when evaluating health communication campaigns, the most successful of which are large, naturally occurring programs that do not constrain people’s exposure to campaign messages—and so, by definition, cannot be assessed via a rigid randomized trial design (Hornik, <link linkend="ch0032s0000li0040">2002</link>). Thus, researchers aiming to evaluate media campaigns need to use alternative designs that respect the way communication actually flows through social systems, while still limiting threats to validity and maximizing the potential for causal inference (e.g., true and constructed cohort studies, time series designs; Hornik, <link linkend="ch0032s0000li0040">2002</link>). Such designs do not enable the same causal claims as trials, but they can get close while still allowing identification of campaign‐driven health behavior change.</para>
   <para id="ch0032s0000p0045">There are also ongoing challenges in measuring key constructs. Perhaps most salient is the question of how best to capture media exposure—an important construct, as it is often the central independent variable in media effects and health communication research. After all, if someone is not exposed to media content, whether directly or indirectly, then they cannot be influenced by it. Measuring media exposure is becoming increasingly complex given the ever‐growing number and breadth of information sources to which the public is exposed, from social media to digital advertising to streaming TV services. Over the years there have been several syntheses of and validity studies on measuring media exposure, including a special issue of<emphasis>Communication Methods and Measures</emphasis> dedicated to this topic in 2016 (De Vreese &amp; Neijens, <link linkend="ch0032s0000li0017">2016</link>). Such work has summarized the breadth of measures available, their strengths and limitations, and their applications. It has also provided evidence of measures that perform best against a set of validity criteria (e.g., nomological validity, discriminant validity). Also, these reviews have laid out directions for future research on measure development.</para>
   <para id="ch0032s0000p0046">Finally, there is the pressing challenge of developing an evidence base for effective communication interventions to address phenomena such as conflicting scientific information in the media, mis/disinformation, and politicization of health and science issues—issues that predated but also became more urgent with COVID‐19. The extant evidence for such interventions is stronger in some areas than others. For instance, there has been considerable work on strategies to mitigate and even prevent the adverse consequences of exposure to mis/disinformation, much (though certainly not all) of it spurred by the COVID‐19 pandemic (see, for example, van der <ulink type="drugsynonym" url="/search/search_results_index.aspx?searchterm=Linden">Linden</ulink>,<link linkend="ch0032s0000li0076">2022</link>). In contrast, little is known about how best to limit the now well‐documented undesirable effects of exposure to conflicting health information, including worrisome downstream carryover (or spillover) effects of such information on people’s receptivity to unrelated health messages for which there is scientific consensus (e.g., fruit and vegetable consumption, physical activity) (Nagler et al., <link linkend="ch0032s0000li0059">2021</link>). And although there is some initial evidence that forewarning messages could guard or “inoculate” against the negative effects of exposure to politicized information (Fowler et al., <link linkend="ch0032s0000li0026">2022</link>), research in this space is nascent.</para>
</sect1>
